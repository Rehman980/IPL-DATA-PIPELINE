# IPL Data Pipeline

A production-grade ETL pipeline for processing IPL match data with PySpark, Google Cloud Storage, and BigQuery.

## ğŸ“Œ Overview

This pipeline processes IPL match data from:
- Match information (teams, venues, results)
- Ball-by-ball delivery data

Key features:
- Incremental processing of new data
- 8 analytical transformations
- GCS â†’ BigQuery â†’ PySpark â†’ BigQuery/GCS flow
- Production-ready with logging, validation, and watermark tracking

## ğŸ› ï¸ Tech Stack

- **Core**: Python 3.8+
- **Processing**: PySpark 3.3
- **Cloud Services**:
  - Google Cloud Storage (GCS)
  - BigQuery
- **Utilities**:
  - Python-dotenv (configuration)
  - Pytest (testing)

## ğŸ“‚ Project Structure

ipl-data-pipeline/
â”œâ”€â”€ config/ # Configuration files
â”œâ”€â”€ connectors/ # GCS and BigQuery connectors
â”œâ”€â”€ data_models/ # Data schemas
â”œâ”€â”€ etl/ # Pipeline components
â”‚ â”œâ”€â”€ extract/ # Data extraction
â”‚ â”œâ”€â”€ transform/ # All transformations
â”‚ â””â”€â”€ load/ # Data loading
|-- input_files/ # Contains input files for upload and testing
| |__ combined_data/ # All the data  
| |__ saperated_data/ # saperate data for each file will be stored here
â”œâ”€â”€ utils/ # Helper functions
â”œâ”€â”€ tests/ # Unit tests
â”œâ”€â”€ .env # Environment variables
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ main.py # Pipeline entry point


## ğŸ”§ Setup Instructions

### Prerequisites
- Google Cloud account with:
  - GCS bucket
  - BigQuery dataset
  - Service account credentials
- Python 3.8+
- Java 8 (for PySpark)

### Installation
1. Clone the repository:
   ```bash
   git clone https://github.com/Rehman980/IPL-DATA-PIPELINE.git
   cd IPL-DATA-PIPELINE

2. Create and activate virtual environment:
```
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate  
```